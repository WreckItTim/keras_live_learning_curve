import tensorflow as tf
from sklearn.datasets import load_iris
from sklearn.preprocessing import OneHotEncoder

from IPython.display import clear_output
import matplotlib.pyplot as plt
class learning_curve(tf.keras.callbacks.Callback):
    def __init__(self, track_metrics=['loss'], ylabel='Metric'):
        self.epochs = []
        self.metrics = {metric:[] for metric in track_metrics}
        self.ylabel = ylabel
    def on_epoch_end(self, epoch, logs=None):
        clear_output(wait=True)
        self.epochs.append(len(self.epochs))
        for metric in self.metrics:
            self.metrics[metric].append(logs.get(metric))
            plt.plot(self.epochs, self.metrics[metric], label=metric)
        plt.title('Learning Curve')
        plt.xlabel('Epoch')
        plt.ylabel(self.ylabel)
        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))
        plt.show()

# load iris toy dataset
data = load_iris()
X, Y = data['data'], data['target']

# one hot encode
Y = OneHotEncoder(sparse=False).fit_transform(Y.reshape([-1, 1]))

# create simple model
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(3))
model.compile(loss='mean_squared_error', optimizer='adam')
history = model.fit(X, Y,
                    epochs=100,
                    callbacks=[learning_curve(track_metrics=['loss'])], 
                    verbose=0,
                   )
